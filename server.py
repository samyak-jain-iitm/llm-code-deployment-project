import os
import json
import logging
import tempfile
import base64
import shutil
import time
from typing import List, Optional

import requests
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel

from generator import generate_project_files
from github_utils import *

from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("llm-deploy")


# Required env vars:
# - STUDENT_SECRET (the secret you provided via Google Form â€” used to verify incoming requests)
# - GITHUB_TOKEN (personal access token with repo or pages scopes)
EXPECTED_SECRET = os.environ.get("STUDENT_SECRET")
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
owner = os.environ.get("GITHUB_OWNER")

if not GITHUB_TOKEN:
    logger.warning("GITHUB_TOKEN not set. The server will fail to create repos until it is provided.")

app = FastAPI(title="LLM Code Deployment Project")


class Attachment(BaseModel):
    name: str
    url: str


class TaskRequest(BaseModel):
    email: str
    secret: str
    task: str
    round: int
    nonce: str
    brief: str
    checks: List[str]
    evaluation_url: str
    attachments: Optional[List[Attachment]] = []


@app.post("/api-endpoint")
async def api_endpoint(req: TaskRequest, background_tasks: BackgroundTasks):
    """
    Accepts the JSON task request described in the spec.
    Verifies secret and schedules background work.
    Responds HTTP 200 quickly (spec requirement) and continues building in background.
    """
    if EXPECTED_SECRET is None:
        raise HTTPException(status_code=500, detail="Server misconfigured: STUDENT_SECRET not set")

    if req.secret != EXPECTED_SECRET:
        logger.warning("Invalid secret for email=%s task=%s", req.email, req.task)
        raise HTTPException(status_code=401, detail="invalid secret")

    # Immediately respond with HTTP 200 JSON
    logger.info("Accepted task request: email=%s task=%s round=%s", req.email, req.task, req.round)

    # schedule background processing (build + deploy + notify)
    background_tasks.add_task(process_request_and_deploy, req.dict())
    return {"status": "accepted", "round": req.round}


def process_request_and_deploy(payload: dict):
    """
    Long-running background work:
    - parse attachments
    - generate minimal app (LLM or templates)
    - create repo and files
    - enable pages
    - notify evaluation_url with repo metadata
    """
    start_ts = time.time()
    tempdir = tempfile.mkdtemp(prefix="llm-deploy-")
    try:
        logger.info("Processing task in %s", tempdir)
        # 1) Save attachments
        attachments = payload.get("attachments", []) or []

        # 2) Generate files (LLM-assisted or template fallback)
        round_num = payload.get("round", 1)
        repo_name = payload["task"].replace(" ", "-")
        files = generate_project_files(
            brief=payload["brief"],
            attachments=attachments,
            checks=payload.get("checks", []),
            task=payload["task"],
            repo_name=repo_name,
            round_num=round_num
        )
        # ensure README and LICENSE exist
        if "README.md" not in files:
            files["README.md"] = f"# {payload['task']}\n\nAuto-generated by LLM Code Deployment.\n"
        if "LICENSE" not in files:
            files["LICENSE"] = files.get("LICENSE", '')  # generator should put MIT; fallback empty

        # 3) Create a unique repo name and push files
        # sanitize task to repo-safe name
        # create or update repo depending on round
        if int(payload.get("round", 1)) == 1:
            # Round 1 -> create new repo
            repo_info = create_repo_and_push(
                token=os.environ.get("GITHUB_TOKEN"),
                repo_name=repo_name,
                files=files
            )
        else:
            # Round >= 2 -> update existing repo; merge README and update/create other files.
            try:
                repo_info = update_repo_and_push(
                    token=os.environ.get("GITHUB_TOKEN"),
                    repo_name=repo_name,
                    files=files,
                    owner=None,  # optional: you can pass an owner if you know it
                    round_num=int(payload.get("round", 2)),
                )
            except Exception as e:
                # If update fails (repo missing or permissions), fall back to creating a new repo
                logger.exception("Update failed for %s: %s. Falling back to creating a new repo.", repo_name, e)
                fallback_name = f"{repo_name}-r{payload.get('round')}"
                repo_info = create_repo_and_push(
                    token=os.environ.get("GITHUB_TOKEN"),
                    repo_name=fallback_name,
                    files=files
                )

        repo_url = repo_info["repo_url"]
        commit_sha = repo_info["commit_sha"]

        # 4) Enable GitHub Pages (idempotent)
        repo_html_name = repo_url.rstrip("/").split("/")[-1]

        pages_resp = deploy_github_pages(owner, repo_html_name, token=os.environ.get("GITHUB_TOKEN"), round_num=round_num)

        # compute pages_url from repo info (handles fallback repo names)
        pages_url = f"https://{owner}.github.io/{repo_html_name}/"
        logger.info("Requested pages: %s -> %s", repo_url, pages_url)

        # 5) Wait for Pages to be reachable (small loop)
        if not wait_for_pages_ready(pages_url, timeout_seconds=180):
            logger.warning("Pages not reachable within timeout: %s", pages_url)

        # 6) Notify evaluation_url within 10 minutes (with backoff on error)
        notify_payload = {
            "email": payload["email"],
            "task": payload["task"],
            "round": payload["round"],
            "nonce": payload["nonce"],
            "repo_url": repo_url,
            "commit_sha": commit_sha,
            "pages_url": pages_url,
        }

        headers = {"Content-Type": "application/json"}
        success = post_with_backoff(payload["evaluation_url"], notify_payload, headers=headers, max_attempts=8)
        if not success:
            logger.error("Failed to get HTTP 200 from evaluation_url after retries: %s", payload["evaluation_url"])
        else:
            logger.info("Evaluation server acknowledged submission for %s", repo_url)

    except Exception as e:
        logger.exception("Error processing request: %s", e)
    finally:
        try:
            shutil.rmtree(tempdir)
        except Exception:
            pass
